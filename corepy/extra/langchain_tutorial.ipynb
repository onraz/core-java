{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Tutorial: Step-by-Step\n",
        "\n",
        "Welcome to this step-by-step tutorial on LangChain! We'll go from the basics to more advanced concepts.\n",
        "\n",
        "## 1. Hello World: Your First LLM Application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Instantiate the model. We're using chat-gpt-3.5-turbo for this tutorial.\n",
        "# Make sure your OPENAI_API_KEY is set in your environment variables.\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# The invoke method sends a single request to the model.\n",
        "# The input is a list of messages, in this case, just one from a human.\n",
        "response = llm.invoke(\"What is the capital of France?\")\n",
        "\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prompt Templates: Managing User Input\n",
        "\n",
        "Hardcoding the prompt like we did above is not very flexible. In a real application, you'll want to take user input and format it into a prompt.\n",
        "\n",
        "LangChain provides **Prompt Templates** for this. They are recipes for generating prompts and can include instructions, few-shot examples, and user input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that provides information about countries.\"),\n",
        "    (\"human\", \"What is the capital of {country}?\"),\n",
        "])\n",
        "\n",
        "# We can format the prompt by passing in the variables.\n",
        "prompt = template.format(country=\"the United States\")\n",
        "\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Chains: Combining Prompts and Models\n",
        "\n",
        "So far, we've created a prompt and we have a model. A **Chain** is what lets us combine these two things.\n",
        "\n",
        "The most common type of chain takes a prompt template, formats it with user input, and then sends the formatted prompt to a model.\n",
        "\n",
        "LangChain has a special syntax for this called the LangChain Expression Language (LCEL), which uses the pipe `|` operator to chain components together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that provides information about countries.\"),\n",
        "    (\"human\", \"What is the capital of {country}?\"),\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# We create the chain by piping the template to the language model.\n",
        "chain = template | llm\n",
        "\n",
        "# We can now invoke the chain with the input variables.\n",
        "# The chain will first format the prompt and then send it to the LLM.\n",
        "response = chain.invoke({\"country\": \"Japan\"})\n",
        "\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agents and Tools: Giving LLMs Access to the World\n",
        "\n",
        "An **Agent** is a system that uses an LLM as a reasoning engine to decide what actions to take. A **Tool** is a function that the agent can call to get information from the outside world (e.g., a search engine, a calculator, a database).\n",
        "\n",
        "This is one of the most powerful features of LangChain, as it allows LLMs to interact with external systems and answer questions about data they weren't trained on.\n",
        "\n",
        "For this example, we'll need to install a new package, `duckduckgo-search`, to use as a tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain import hub\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1. Create a tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "tools = [search]\n",
        "\n",
        "# 2. Create an agent\n",
        "# We're using a prompt from the LangChain Hub. This is a pre-made prompt for a ReAct agent.\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "\n",
        "# 3. Create an agent executor\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# 4. Invoke the agent\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": \"Who is the current prime minister of the UK and what is their age?\"\n",
        "})\n",
        "\n",
        "print(response[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Memory: Remembering Past Interactions\n",
        "\n",
        "To build conversational applications like chatbots, your chains and agents need to have **Memory**.\n",
        "\n",
        "LangChain provides various types of memory. The simplest one is `ConversationBufferMemory`, which just stores the conversation history as a list of messages.\n",
        "\n",
        "Let's see how to add memory to a chain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Output Parsers: Structuring LLM Responses\n",
        "\n",
        "Often, you want the LLM to return structured data (like JSON, lists, or Python objects) instead of just plain text. **Output Parsers** help you define the structure you want and automatically parse the LLM's output.\n",
        "\n",
        "Let's see how to use Pydantic-based output parsers to get structured data from an LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# 1. Define the structure you want using Pydantic\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"The person's name\")\n",
        "    age: int = Field(description=\"The person's age\")\n",
        "    occupation: str = Field(description=\"The person's occupation\")\n",
        "    \n",
        "# 2. Create a model that can output structured data\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Use with_structured_output to make the LLM return structured data\n",
        "structured_llm = llm.with_structured_output(Person)\n",
        "\n",
        "# 3. Create a prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert at extracting information from text.\"),\n",
        "    (\"human\", \"Extract the person's information from the following text: {text}\")\n",
        "])\n",
        "\n",
        "# 4. Create the chain\n",
        "chain = prompt | structured_llm\n",
        "\n",
        "# 5. Invoke with some text\n",
        "response = chain.invoke({\n",
        "    \"text\": \"John Smith is a 35-year-old software engineer living in San Francisco.\"\n",
        "})\n",
        "\n",
        "print(f\"Name: {response.name}\")\n",
        "print(f\"Age: {response.age}\")\n",
        "print(f\"Occupation: {response.occupation}\")\n",
        "print(f\"Type: {type(response)}\")  # This is a Person object!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG (Retrieval Augmented Generation): Teaching LLMs About Your Data\n",
        "\n",
        "**RAG** is a technique that allows LLMs to answer questions about your own documents or data. It works by:\n",
        "1. **Splitting** your documents into chunks\n",
        "2. **Embedding** those chunks (converting them to vectors)\n",
        "3. **Storing** them in a vector database\n",
        "4. **Retrieving** relevant chunks when a question is asked\n",
        "5. **Passing** those chunks to the LLM as context\n",
        "\n",
        "This is incredibly useful for building chatbots over your documentation, research papers, internal wikis, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Create some sample documents\n",
        "documents = [\n",
        "    \"LangChain is a framework for developing applications powered by language models.\",\n",
        "    \"It provides tools for prompt management, chains, agents, and memory.\",\n",
        "    \"LangChain was created by Harrison Chase in October 2022.\",\n",
        "    \"The framework supports multiple language models including OpenAI, Anthropic, and Hugging Face models.\",\n",
        "    \"RAG stands for Retrieval Augmented Generation and is a popular pattern in LangChain.\",\n",
        "]\n",
        "\n",
        "# 2. Split documents into chunks (in this case, they're already small)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "splits = text_splitter.create_documents(documents)\n",
        "\n",
        "# 3. Create embeddings and store in a vector database\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# 4. Create a retriever from the vectorstore\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# 5. Create a prompt template for RAG\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# 6. Create the RAG chain using LCEL\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 7. Ask a question\n",
        "response = rag_chain.invoke(\"Who created LangChain?\")\n",
        "print(response)\n",
        "\n",
        "# Ask another question\n",
        "response = rag_chain.invoke(\"What does RAG stand for?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Streaming: Real-Time Response Generation\n",
        "\n",
        "For better user experience in chatbots and interactive applications, you often want to **stream** the LLM's response as it's being generated (like ChatGPT does), rather than waiting for the entire response.\n",
        "\n",
        "LangChain makes streaming easy with the `.stream()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import sys\n",
        "\n",
        "# Create a chain\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short story about {topic}\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Use .stream() instead of .invoke()\n",
        "print(\"Streaming response:\\n\")\n",
        "for chunk in chain.stream({\"topic\": \"a robot learning to paint\"}):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "    \n",
        "print(\"\\n\\nDone!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1. Create a model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# 2. Create a conversation chain\n",
        "# We set verbose=True to see the prompt being sent to the LLM.\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# 3. Start the conversation\n",
        "response = conversation.predict(input=\"Hi, I'm Bob.\")\n",
        "print(response)\n",
        "\n",
        "# 4. Continue the conversation\n",
        "response = conversation.predict(input=\"What's my name?\")\n",
        "print(response)\n",
        "\n",
        "# 5. Let's ask another question\n",
        "response = conversation.predict(input=\"What's the weather like in London today?\")\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
